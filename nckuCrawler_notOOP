import re
import sys
import json
import requests
from time import sleep
from bs4 import BeautifulSoup  


l =[]
start = int(sys.argv[1])
end = int(sys.argv[2])

#自定清除格式1
def format(unformat):
	try:
		format = unformat.encode('raw_unicode_escape').decode('utf-8').replace(' ', '').replace('\n', '').replace('\t', '') 
		return format
	except:
		pass

#自定清除格式1	
def format2(unformat):
	try:
		format = unformat.encode('raw_unicode_escape').decode('utf-8').replace('\n', '').replace('\t', '') 
		return format
	except:
		pass

def storage(l):
	json_data = json.dumps(l,ensure_ascii=False)
	with open('ncku.json', 'w') as f:
		f.write(json_data)

def crawler(start,end):
	for key in range(start,end):
		print( 'index is '+str(key) )
		url = "http://web.ncku.edu.tw/files/501-1000-1048-"+str(key)+".php"
		resp = requests.get(url=url)
		soup = BeautifulSoup(resp.text)
		for tag in soup.find_all("tr",class_=re.compile("row_0")):	
			date = format( tag.contents[1].string )
			title = format2( tag.find("a") )
			organization = format( tag.contents[5].string )
			dic = {"日期":date,"標題":title,"公告單位":organization} 
			print(dic)
			l.append(dic)
			sleep(0.1)
		sleep(3)
	storage(l)

crawler(start,end)

